# -*- coding: utf-8 -*-
"""VAE_2 (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XreqlH9AXvqQ3L2e5QZxpVI1AuMKarEn
"""

from google.colab import drive
drive.mount('/content/drive',force_remount=True)

import os
os.chdir('/content/drive/My Drive/Deep_Learning_hw2/VAE_dataset')

import sys
import torch
import json
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import torch.nn.functional as F
from PIL import Image
import PIL
import torchvision.transforms as transforms
from torch import nn, optim
from torch.utils import data
from torch.utils.data import Dataset, DataLoader
from sklearn.utils import shuffle
import torchvision.utils
from torchvision.utils import save_image

class Dataset():
    def __init__(self):
        self.pic_name = os.listdir('data')  
        self.loader = transforms.Compose([transforms.Resize(size=(64, 64)),transforms.ToTensor()])
    def __getitem__(self,index): 
        image = Image.open('./data/'+self.pic_name[index])
        image_tensor = self.loader(image)

        return image_tensor,torch.tensor([1])
    def pad_batch(self,batch):
        (image_tensor,label_tensor) = zip(*batch)
        images = torch.stack(image_tensor)
        labels = torch.stack(label_tensor)
        return images,labels
    def __len__(self):
        return len(self.pic_name)


class VAE(nn.Module):
    def __init__(self):
        super(VAE, self).__init__()

        self.cnn0 = nn.Sequential(
            nn.Conv2d(3, 16, 3, 1, 1),  # [64, 64, 64]
            nn.BatchNorm2d(16),
            nn.ReLU(),
            nn.MaxPool2d(2, 2, 0),      # [64, 32, 32]

            nn.Conv2d(16, 32, 3, 1, 1), # [32, 32, 32]
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.MaxPool2d(2, 2, 0),      # [32, 16, 16]

            nn.Conv2d(32, 32, 3, 1, 1),  # [8, 16, 16]
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.MaxPool2d(2, 2, 0),      # [32, 8, 8]

        )


        self.fc0 = nn.Sequential(

            nn.Linear(2048, 512),
            nn.LeakyReLU(negative_slope= 1/32),
            nn.Linear(512, 40)
        )


        
        self.fc1 = nn.Sequential(
            nn.Linear(20,256),
            nn.LeakyReLU(negative_slope= 1/32),
            nn.Linear(256,2048),
            nn.LeakyReLU(negative_slope= 1/32),
            nn.Linear(2048,4096),
            nn.LeakyReLU(negative_slope= 1/32),
            nn.Linear(4096,12288),
            nn.Sigmoid()
        )

    def encoder(self,x):
        out = self.cnn0(x)
        out = out.view(out.size()[0], -1)
        return self.fc0(out)


    def decoder(self,z):
        out = self.fc1(z)
        out  = out.view(out.size()[0],3,64,64)
        return out
    
    
    def forward(self,x):
        mean,log_var = torch.split(self.encoder(x),20,dim = 1)
        eps = torch.randn_like(log_var)

        z = mean + log_var.exp()*eps
        
        return self.decoder(z),mean,log_var

def draw_chart(chart_data,outfile_name):
    # -------------- draw loss image --------------
    # -------------- new one figure and set resolution --------------
    plt.figure(figsize=(12.0, 6.0))
    plt.rcParams['savefig.dpi'] = 200
    plt.rcParams['figure.dpi'] = 200
    # -------------- plot data in image --------------
    plt.plot(chart_data['epoch'],chart_data['train_loss'],label='train_loss')
    # -------------- draw underline --------------
    plt.grid(True,axis="y",ls='--')
    # -------------- draw legent --------------
    plt.legend(loc= 'best')
    # -------------- show lable --------------
    plt.xlabel('epoch',fontsize=20)
    # plt.yticks(np.linspace(0,1,11))
    # plt.savefig('./data2/'+outfile_name+'_train_loss.jpg')
    # plt.close('all')
    plt.show()

    with open('./data2/'+outfile_name+'.json','w') as file_object:
        json.dump(chart_data,file_object)

def interpolation(img1,img2):
    image_files = []
    z_1 = img1
    z_2 = img2
    gap = (z_2-z_1)/10
    for i in range(1,10):
        z = z_1 + gap*i
        recon_batch = model.decoder(z)
        save = to_img(recon_batch.cpu().data)
        image_files.append(save)
        save_image(save, './'+str(i)+'.png')

    img_width = 64
    img_height = 64
    img_number = 9

    target = Image.new('L', (img_width * img_number, img_height))
    for i,img_tensor in zip(range(11),image_files):
        target.paste(transforms.ToPILImage()(img_tensor[0]), (0+64*i,0))
    target.save('t.png')

def to_img(x):
    x = x.clamp(0, 1)
    x = x.view(x.size(0), 3, 64, 64)
    return x

if __name__ == '__main__':
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    batch_size = 64
    learning_rate = 0.001
    model_name = "vae_model"

    train_set = Dataset()
    train_loader = DataLoader(train_set,batch_size=batch_size,collate_fn=train_set.pad_batch)

    model = VAE()
    model = model.to(device)

    optimizer = optim.Adam(model.parameters(), lr=learning_rate,weight_decay=1e-5)

    chart_data={"train_loss":[],"val_acc":[],"train_acc":[],"epoch":[]}
    criterion = nn.CrossEntropyLoss()
    for epoch in range(100):
        train_loss = 0
        count = 0
        correct = 0
        model.train()
        optimizer.zero_grad()
        for step, (batch) in enumerate(train_loader):

            pic_tensor,_ = [t.to(device) for t in batch]
            decode_vec,mu,log_var = model(pic_tensor)
   
            reconst_loss = F.binary_cross_entropy(decode_vec, pic_tensor.view(-1, 3, 64, 64), size_average=False)
            kl_div = - 0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())


            loss = (reconst_loss + kl_div) / batch_size
            train_loss += loss.item()
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            x_concat = torch.cat([pic_tensor.view(-1, 3, 64, 64), decode_vec.view(-1, 3, 64, 64)], dim=3)
            x_concat2 = torch.cat([decode_vec.view(-1,3,64,64)],dim=3)
            x_concat3 = torch.cat([pic_tensor.view(-1,3,64,64)],dim=3)
            # save_image(x_concat, './data2/'+str(step)+'.png')
            torch.save(model.state_dict(), './data2/'+model_name+'2.pkl')
            if step %1000 == 0:
                print('Epoch: ' , str(epoch) , \
                        '\ttrain_loss: '+str(round(train_loss/(step+1),5)),\
                    )
 
        chart_data['epoch'].append(epoch)
        chart_data['train_loss'].append(train_loss/(step+1))

decode_vec = decode_vec.cpu()
pic_tensor = pic_tensor.cpu()

img2 = torchvision.utils.make_grid(pic_tensor.view(-1, 3, 64, 64))
plt.imshow(img2.permute(1,2,0))
plt.axis('off')
plt.show()


img = torchvision.utils.make_grid(decode_vec.view(-1, 3, 64, 64))
plt.imshow(img.permute(1, 2, 0))
plt.axis('off')
plt.show()

def random_sample():
    sample = torch.randn(20,device="cuda")
    return sample

s = random_sample()
with torch.no_grad():
    model=VAE()
    # model.load_state_dict(torch.load('./data2/'+'vae_model2.pkl'))
    model.eval()
    device="cuda"
    model.to(device)
    decode_s = model.decoder(s)
    x_sample = torch.cat([decode_s.view(-1, 3, 64, 64)], dim=3)
    x_sample = to_img(x_sample).cpu()
    img3 = torchvision.utils.make_grid(x_sample.view(-1, 3, 64, 64))
    plt.imshow(img3.permute(1, 2, 0))
    interpolation(torch(512).normal_().reshape(1,-1).to(device),torch.FloatTensor(512).normal_().reshape(1,-1).to(device))

"""## KL:100"""

if __name__ == '__main__':
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    batch_size = 64
    learning_rate = 0.001
    model_name = "vae_model"

    train_set = Dataset()
    train_loader = DataLoader(train_set,batch_size=batch_size,collate_fn=train_set.pad_batch)

    model = VAE()
    model = model.to(device)

    optimizer = optim.Adam(model.parameters(), lr=learning_rate,weight_decay=1e-5)

    chart_data={"train_loss":[],"val_acc":[],"train_acc":[],"epoch":[]}
    criterion = nn.CrossEntropyLoss()
    for epoch in range(100):
        train_loss = 0
        count = 0
        correct = 0
        model.train()
        optimizer.zero_grad()
        for step, (batch) in enumerate(train_loader):

            pic_tensor,_ = [t.to(device) for t in batch]
            decode_vec,mu,log_var = model(pic_tensor)
   
            reconst_loss = F.binary_cross_entropy(decode_vec, pic_tensor.view(-1, 3, 64, 64), size_average=False)
            kl_div = - 0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())*100


            loss = (reconst_loss + kl_div) / batch_size
            train_loss += loss.item()
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            x_concat = torch.cat([pic_tensor.view(-1, 3, 64, 64), decode_vec.view(-1, 3, 64, 64)], dim=3)
            x_concat2 = torch.cat([decode_vec.view(-1,3,64,64)],dim=3)
            x_concat3 = torch.cat([pic_tensor.view(-1,3,64,64)],dim=3)
            # save_image(x_concat, './data2/'+str(step)+'.png')
            torch.save(model.state_dict(), './data2/'+model_name+'2.pkl')
            if step %1000 == 0:
                print('Epoch: ' , str(epoch) , \
                        '\ttrain_loss: '+str(round(train_loss/(step+1),5)),\
                    )
 
        chart_data['epoch'].append(epoch)
        chart_data['train_loss'].append(train_loss/(step+1))

draw_chart(chart_data,model_name)

decode_vec = decode_vec.cpu()
pic_tensor = pic_tensor.cpu()

img2 = torchvision.utils.make_grid(pic_tensor.view(-1, 3, 64, 64))
plt.imshow(img2.permute(1,2,0))
plt.axis('off')
plt.show()


img = torchvision.utils.make_grid(decode_vec.view(-1, 3, 64, 64))
plt.imshow(img.permute(1, 2, 0))
plt.axis('off')
plt.show()

with torch.no_grad():
    s = random_sample()
    model=VAE()
    # model.load_state_dict('./data2/vae_model.pkl')
    model.eval()
    device="cuda"
    model.to(device)
    decode_s = model.decoder(s)
    x_sample = torch.cat([decode_s.view(-1, 3, 64, 64)], dim=3)
    x_sample = to_img(x_sample).cpu()
    img3 = torchvision.utils.make_grid(x_sample.view(-1, 3, 64, 64))
    plt.imshow(img3.permute(1, 2, 0))
    plt.axis('off')
    plt.show()
    interpolation(torch.FloatTensor(512).normal_().reshape(1,-1).to(device),torch.FloatTensor(512).normal_().reshape(1,-1).to(device))

"""## KL :0"""

if __name__ == '__main__':
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    batch_size = 64
    learning_rate = 0.001
    model_name = "vae_model"

    train_set = Dataset()
    train_loader = DataLoader(train_set,batch_size=batch_size,collate_fn=train_set.pad_batch)

    model = VAE()
    model = model.to(device)

    optimizer = optim.Adam(model.parameters(), lr=learning_rate,weight_decay=1e-5)

    chart_data={"train_loss":[],"val_acc":[],"train_acc":[],"epoch":[]}
    criterion = nn.CrossEntropyLoss()
    for epoch in range(100):
        train_loss = 0
        count = 0
        correct = 0
        model.train()
        optimizer.zero_grad()
        for step, (batch) in enumerate(train_loader):

            pic_tensor,_ = [t.to(device) for t in batch]
            decode_vec,mu,log_var = model(pic_tensor)
   
            reconst_loss = F.binary_cross_entropy(decode_vec, pic_tensor.view(-1, 3, 64, 64), size_average=False)
            kl_div = - 0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp()) *0


            loss = (reconst_loss + kl_div) / batch_size
            train_loss += loss.item()
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            x_concat = torch.cat([pic_tensor.view(-1, 3, 64, 64), decode_vec.view(-1, 3, 64, 64)], dim=3)
            x_concat2 = torch.cat([decode_vec.view(-1,3,64,64)],dim=3)
            x_concat3 = torch.cat([pic_tensor.view(-1,3,64,64)],dim=3)
            # save_image(x_concat, './data2/'+str(step)+'.png')
            torch.save(model.state_dict(), './data2/'+model_name+'2.pkl')
            if step %1000 == 0:
                print('Epoch: ' , str(epoch) , \
                        '\ttrain_loss: '+str(round(train_loss/(step+1),5)),\
                    )
 
        chart_data['epoch'].append(epoch)
        chart_data['train_loss'].append(train_loss/(step+1))

draw_chart(chart_data,model_name)

decode_vec = decode_vec.cpu()
pic_tensor = pic_tensor.cpu()

img2 = torchvision.utils.make_grid(pic_tensor.view(-1, 3, 64, 64))
plt.imshow(img2.permute(1,2,0))
plt.axis('off')
plt.show()


img = torchvision.utils.make_grid(decode_vec.view(-1, 3, 64, 64))
plt.imshow(img.permute(1, 2, 0))
plt.axis('off')
plt.show()

with torch.no_grad():
    s = random_sample()
    model=VAE()
    # model.load_state_dict('./data2/vae_model.pkl')
    model.eval()
    device="cuda"
    model.to(device)
    decode_s = model.decoder(s)
    x_sample = torch.cat([decode_s.view(-1, 3, 64, 64)], dim=3)

    interpolation(torch.FloatTensor(512).normal_().reshape(1,-1).to(device),torch.FloatTensor(512).normal_().reshape(1,-1).to(device))

x_sample = x_sample.cpu()
img3 = torchvision.utils.make_grid(x_sample.view(-1, 3, 64, 64))
plt.imshow(img3.permute(1, 2, 0))
plt.axis('off')
plt.show()

